{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6671c-c40d-433f-a163-198750a3a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import peft\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e07feb-53c5-4a2e-81c3-3ff2c54be7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abf55b-9c7f-435f-9f6f-39d4b01e0943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VAROSDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        for root, _, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                    if 'A' in root:\n",
    "                        image_path = os.path.join(root, file)\n",
    "                        depth_path = os.path.join(root.replace('A', 'D'), file.replace('A', 'D'))\n",
    "                        data.append((image_path, depth_path))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, depth_path = self.data[idx]\n",
    "        image = Image.open(image_path)\n",
    "        depth = Image.open(depth_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            depth = self.transform(depth)\n",
    "        return image, depth\n",
    "\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "DATASET_ROOT_DIR = \"D:\\\\EMJMD MIR\\\\france\\\\ddmp\\\\Varos\\\\2021-08-17_SEQ1\\\\vehicle0\\\\cam0\"\n",
    "\n",
    "dataset = VAROSDataset(root_dir= DATASET_ROOT_DIR, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "images, depths = batch\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10, 20))\n",
    "for i in range(5):\n",
    "    image = transforms.ToPILImage()(images[i])\n",
    "    depth = transforms.ToPILImage()(depths[i])\n",
    "\n",
    "    axes[i, 0].imshow(image)\n",
    "    axes[i, 0].set_title('Image')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(depth, cmap='gray')\n",
    "    axes[i, 1].set_title('Depth')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb89aaf-e2e3-4266-822d-9767b3db2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ab6b6-bf24-4b84-8422-7fa9e3883085",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af01f4b-2c97-4db3-bf99-6f44478f307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef01d06-ca64-4aff-ade5-3fe4d852873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"LiheYoung/depth-anything-small-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee8907-468e-430b-8e7e-7ca9ae14ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66f0573-80ca-405a-a0e8-210324408ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "#normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "data_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        #normalize,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b4fd1-4050-4eff-a527-54318691f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(root_dir='/mundus/abhowmik697/FLSea_Dataset', transform=data_transforms)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dfddc-c722-45f9-8bd3-5c9b3d0d8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22385808-7872-42df-b4b0-5abc5a5f4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497480f-c620-4b0a-a7d2-60cb87761b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForDepthEstimation, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ceb25-52fc-45a2-a6ea-5404b560cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7aac63-10f2-4992-a099-4634f29d157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"lora_only\",\n",
    "    modules_to_save=[\"decode_head\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169ded0-9acf-4fcd-9f79-af2c4b1f275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in lora_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b241d-33ec-43cd-8f46-c022b4d32ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "def scale_offset_invariance_psnr(depth_map1, depth_map2):\n",
    "    # Calculate the mean value of both depth maps\n",
    "    mean1 = np.mean(depth_map1)\n",
    "    mean2 = np.mean(depth_map2)\n",
    "\n",
    "    # Calculate the scale factor\n",
    "    scale_factor = mean1 / mean2\n",
    "\n",
    "    # Adjust the second depth map by the scale factor\n",
    "    depth_map2_scaled = depth_map2 * scale_factor\n",
    "\n",
    "    # Calculate the offset\n",
    "    offset = mean1 - (scale_factor * mean2)\n",
    "\n",
    "    # Adjust the second depth map by the offset\n",
    "    depth_map2_adjusted = depth_map2_scaled + offset\n",
    "\n",
    "\n",
    "    #Calculate the PSNR\n",
    "    psnr_val = psnr(depth_map1, depth_map2_adjusted)\n",
    "\n",
    "    return psnr_val\n",
    "\n",
    "\n",
    "def scale_offset_invariance_ssim(depth_map1, depth_map2):\n",
    "    # Calculate the mean value of both depth maps\n",
    "    mean1 = np.mean(depth_map1)\n",
    "    mean2 = np.mean(depth_map2)\n",
    "\n",
    "    # Calculate the scale factor\n",
    "    scale_factor = mean1 / mean2\n",
    "\n",
    "    # Adjust the second depth map by the scale factor\n",
    "    depth_map2_scaled = depth_map2 * scale_factor\n",
    "\n",
    "    # Calculate the offset\n",
    "    offset = mean1 - (scale_factor * mean2)\n",
    "\n",
    "    # Adjust the second depth map by the offset\n",
    "    depth_map2_adjusted = depth_map2_scaled + offset\n",
    "\n",
    "    # Calculate the SSIM\n",
    "    ssim_value, _ = ssim(depth_map1, depth_map2_adjusted, full=True)\n",
    "\n",
    "    return ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcf29a-3cbc-4263-a0be-1ea03b9f17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        outputs, gt_depth = eval_pred\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "        \n",
    "        # Convert depth prediction to numpy array and resize to match ground truth depth map size\n",
    "        depth_output = prediction.squeeze().cpu().numpy()\n",
    "        gt_depth = gt_depth.cpu().numpy()\n",
    "\n",
    "        # Handle invalid or unexpected depth values\n",
    "        depth_output[depth_output <= 0] = 1e-7  # Replace negative or zero values with a small epsilon\n",
    "\n",
    "        # Calculate metrics\n",
    "        non_zero_mask = depth_output != 0\n",
    "        absrel = np.mean(np.abs(depth_output[non_zero_mask] - gt_depth[non_zero_mask]) / gt_depth[non_zero_mask])\n",
    "\n",
    "        d = np.log(gt_depth + 1e-7) - np.log(depth_output + 1e-7)\n",
    "        silog = np.mean(np.square(d)) - np.square(np.sum(d))/ np.square(d.size)\n",
    "        pearson_corr = scipy.stats.pearsonr(depth_output.flatten(), gt_depth.flatten())[0]\n",
    "        psnr_val = scale_offset_invariance_psnr(gt_depth,depth_output)\n",
    "        ssim_val = scale_offset_invariance_ssim(gt_depth,depth_output)\n",
    "\n",
    "        metrics.update({\"Absolute Relative error (AbsRel)\":absrel})\n",
    "        metrics.update({\"Scale Invarience MSE (Logscale)\": silog})\n",
    "        metrics.update({\"Pearson Correlation\": pearson_corr})\n",
    "        metrics.update({\"PSNR (Scale and offset Invarience)\": psnr_val})\n",
    "        metrics.update({\"SSIM (Scale and offset Invarience)\":ssim_val})\n",
    "        \n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a0d83-86cc-4175-aaeb-8092ad7de34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    output = lora_model(x)\n",
    "\n",
    "    \n",
    "    loss = loss(output,gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1909f-919a-40f1-908c-8f7818850172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(depth_output,gt_depth):\n",
    "#     d = np.log(gt_depth + 1e-7) - np.log(depth_output + 1e-7)\n",
    "#     silog = np.mean(np.square(d)) - np.square(np.sum(d))/ np.square(d.size)\n",
    "#     return silog\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model = lora_model\n",
    "optimizer = torch.optim.SGD(model.parameters(), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05121efb-01ee-4254-a239-a61e317b4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        print('got data')\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs).predicted_depth\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        print('Computing Loss')\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 16 == 15:\n",
    "            last_loss = running_loss / 16 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            # tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519b0fb-4538-4666-8249-52bf5e4d5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter('runs/depth_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # # Log the running loss averaged per batch\n",
    "    # # for both training and validation\n",
    "    # writer.add_scalars('Training vs. Validation Loss',\n",
    "    #                 { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "    #                 epoch_number + 1)\n",
    "    # writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337610d-e2b1-4a92-918f-856e461710d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a71757-bb11-4366-a077-ceda3c5b3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=\"peft_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e94482-7a24-44ab-bfa7-78cb933ba8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./depth-anything-small-lora\",\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=25,\n",
    "    weight_decay = 0.001,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = 0.03,\n",
    "    max_grad_norm = 0.3,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to = \"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51386706-05b6-476c-8983-04bcc36d010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "class CustomDatasetBuilder:\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = {\"image\": [], \"depth\": []}\n",
    "        for root, _, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.tif'):\n",
    "                    if \"depth\" in root:\n",
    "                        depth_path = os.path.join(root, file)\n",
    "                        image_path = os.path.join(root.replace('depth', 'imgs'), file.replace('_SeaErra_abs_depth.tif', '.tiff'))\n",
    "                        data[\"image\"].append(Image.open(image_path))\n",
    "                        data[\"depth\"].append(Image.open(depth_path))\n",
    "        return data\n",
    "\n",
    "    def get_dataset(self):\n",
    "        data_dict = self._load_data()\n",
    "        return Dataset.from_dict(data_dict)\n",
    "\n",
    "# Example usage\n",
    "builder = CustomDatasetBuilder(root_dir='/mundus/abhowmik697/FLSea_Dataset')\n",
    "dataset = builder.get_dataset().train_test_split(test_size=0.2)\n",
    "print(dataset)\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52761c14-ef67-4560-a23e-6839ba508db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa08dfe-06a0-4c1b-bd2c-b4665a028d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b9fa6-e5c2-42a2-80a2-9a101120d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(data['image']) * 80/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ace68f-1789-4339-88ac-a62d5f67e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = data['image'][:round(len(data['image']) * 80/100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e63b7-c9d8-46f8-9cc3-921d095dcd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16397185-f188-4365-9d30-917d20262698",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08d0b9-33e7-46d5-9aa3-2a8ec51d2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='/mundus/abhowmik697/FLSea_Dataset'\n",
    "data = {\"image\": [], \"depth\": []}\n",
    "for root, _, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.tif'):\n",
    "            if \"depth\" in root:\n",
    "                depth_path = os.path.join(root, file)\n",
    "                image_path = os.path.join(root.replace('depth', 'imgs'), file.replace('_SeaErra_abs_depth.tif', '.tiff'))\n",
    "                data[\"image\"].append(image_path)\n",
    "                data[\"depth\"].append(depth_path)\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset, DatasetDict, Image\n",
    "image_paths_train = data['image'][:round(len(data['image']) * 80/100)]\n",
    "label_paths_train = data['depth'][:round(len(data['depth']) * 80/100)]\n",
    "\n",
    "# same for validation\n",
    "image_paths_validation = data['image'][round(len(data['image']) * 80/100):]\n",
    "label_paths_validation = data['depth'][round(len(data['depth']) * 80/100):]\n",
    "\n",
    "def create_dataset(image_paths, label_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n",
    "                                \"depth\": sorted(label_paths)})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"depth\", Image())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# step 1: create Dataset objects\n",
    "train_dataset = create_dataset(image_paths_train, label_paths_train)\n",
    "validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n",
    "\n",
    "# step 2: create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "  }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13206155-d8b1-43aa-83a8-47ab8b2da856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "# def pad_if_smaller(img, size, fill=0):\n",
    "#     size = (size, size) if isinstance(size, int) else size\n",
    "#     original_width, original_height = img.size\n",
    "#     pad_height = size[1] - original_height if original_height < size[1] else 0\n",
    "#     pad_width = size[0] - original_width if original_width < size[0] else 0\n",
    "#     img = functional.pad(img, (0, 0, pad_width, pad_height), fill=fill)\n",
    "#     return img\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Identity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Resize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = functional.resize(image, self.size)\n",
    "        target = functional.resize(target, self.size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomResize:\n",
    "    def __init__(self, min_size, max_size=None):\n",
    "        self.min_size = min_size\n",
    "        if max_size is None:\n",
    "            max_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        size = random.randint(self.min_size, self.max_size)\n",
    "        image = functional.resize(image, size)\n",
    "        target = functional.resize(target, size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# class RandomCrop:\n",
    "#     def __init__(self, size):\n",
    "#         self.size = size if isinstance(size, tuple) else (size, size)\n",
    "\n",
    "#     def __call__(self, image, target):\n",
    "#         image = pad_if_smaller(image, self.size)\n",
    "#         target = pad_if_smaller(target, self.size, fill=255)\n",
    "#         crop_params = transforms.RandomCrop.get_params(image, self.size)\n",
    "#         image = functional.crop(image, *crop_params)\n",
    "#         target = functional.crop(target, *crop_params)\n",
    "#         return image, target\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, flip_prob):\n",
    "        self.flip_prob = flip_prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.flip_prob:\n",
    "            image = functional.hflip(image)\n",
    "            target = functional.hflip(target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class PILToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = functional.pil_to_tensor(image)\n",
    "        target = torch.as_tensor(np.array(target), dtype=torch.int64)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ConvertImageDtype:\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = functional.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = functional.normalize(image, mean=self.mean, std=self.std)\n",
    "        return image, target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668cc3a-fcd0-46d0-bba0-768f2863709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"shortest_edge\" in image_processor.size:\n",
    "    # We instead set the target size as (shortest_edge, shortest_edge) to here to ensure all images are batchable.\n",
    "    size = (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n",
    "else:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            \n",
    "            RandomHorizontalFlip(flip_prob=0.5),\n",
    "            PILToTensor(),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size=size),\n",
    "            PILToTensor(),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11457c-6447-4839-95be-15b5a224a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(example_batch):\n",
    "        pixel_values = []\n",
    "        labels = []\n",
    "        for image, target in zip(example_batch[\"image\"], example_batch[\"depth\"]):\n",
    "            image, target = train_transforms(image.convert(\"RGB\"), target)\n",
    "            pixel_values.append(image)\n",
    "            labels.append(target)\n",
    "\n",
    "        encoding = {}\n",
    "        encoding[\"pixel_values\"] = torch.stack(pixel_values)\n",
    "        encoding[\"labels\"] = torch.stack(labels)\n",
    "\n",
    "        return encoding\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "        pixel_values = []\n",
    "        labels = []\n",
    "        for image, target in zip(example_batch[\"image\"], example_batch[\"depth\"]):\n",
    "            image, target = val_transforms(image.convert(\"RGB\"), target)\n",
    "            pixel_values.append(image)\n",
    "            labels.append(target)\n",
    "\n",
    "        encoding = {}\n",
    "        encoding[\"pixel_values\"] = torch.stack(pixel_values)\n",
    "        encoding[\"labels\"] = torch.stack(labels)\n",
    "\n",
    "        return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a192f2-7a47-4215-b465-8531501bfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    if \"train\" not in dataset:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    # if data_args.max_train_samples is not None:\n",
    "    #     dataset[\"train\"] = (\n",
    "    #         dataset[\"train\"].shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\n",
    "    #     )\n",
    "    # Set the training transforms\n",
    "    dataset[\"train\"].set_transform(preprocess_train)\n",
    "\n",
    "\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if \"validation\" not in dataset:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    # if data_args.max_eval_samples is not None:\n",
    "    #     dataset[\"validation\"] = (\n",
    "    #         dataset[\"validation\"].shuffle(seed=training_args.seed).select(range(data_args.max_eval_samples))\n",
    "    #     )\n",
    "    # Set the validation transforms\n",
    "    dataset[\"validation\"].set_transform(preprocess_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b371e6c-a1ce-4664-8d4f-5a6cce917c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].set_transform(preprocess_train)\n",
    "dataset[\"validation\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c53153-c72f-440f-8d49-0ab5353b92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForSemanticSegmentation,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e242a7-325e-4d42-936a-9a6483c63e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "#normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "data_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        #normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_ds.set_transform(data_transforms)\n",
    "test_ds.set_transform(data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70480fa2-34a7-46b2-81d5-a8911b075c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a3d3a-3342-4d40-944b-5ffcc7dbb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.transforms import RandomResizedCrop, RandomHorizontalFlip, ToTensor, Compose\n",
    "\n",
    "class CustomDatasetBuilder:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = Compose([\n",
    "            RandomResizedCrop(224),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor()\n",
    "        ])\n",
    "        self.data = self._load_data()\n",
    "        \n",
    "\n",
    "    def _load_data(self):\n",
    "        data = {\"image\": [], \"depth\": []}\n",
    "        for root, _, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.tif'):\n",
    "                    if \"depth\" in root:\n",
    "                        depth_path = os.path.join(root, file)\n",
    "                        image_path = os.path.join(root.replace('depth', 'imgs'), file.replace('_SeaErra_abs_depth.tif', '.tiff'))\n",
    "                        data[\"image\"].append(self.transform(Image.open(image_path)))\n",
    "                        data[\"depth\"].append(self.transform(Image.open(depth_path)))\n",
    "        return data\n",
    "\n",
    "    def get_dataset(self):\n",
    "        data_dict = self._load_data()\n",
    "        return Dataset.from_dict(data_dict)\n",
    "\n",
    "# Example usage\n",
    "builder = CustomDatasetBuilder(root_dir='/mundus/abhowmik697/FLSea_Dataset')\n",
    "dataset = builder.get_dataset()\n",
    "\n",
    "# Split the dataset into train and test sets using scikit-learn's train_test_split function\n",
    "train_ds, test_ds = dataset.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6899a48-a0af-4c51-9c94-6c89ac2492f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5e15f-200c-4166-9a91-dc60e0cd057d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
